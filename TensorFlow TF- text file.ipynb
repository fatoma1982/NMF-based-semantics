{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#! /usr/bin/python3.5\n",
    "#https://learningtensorflow.com/ReadingFilesBasic/\n",
    "#https://www.tensorflow.org/tutorials/using_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#To compute term frequancy from multi txt file and save it in multi csv and one csv by parellel process using tensorflow on GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import csv\n",
    "from tensorflow.python.client import timeline\n",
    "import nltk\n",
    "from nltk.wsd import lesk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.wsd import lesk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import PorterStemmer\n",
    "from nltk.stem.porter import *\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from string import digits\n",
    "#from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pathes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dir_path = os.path.dirname(os.path.realpath(__file__))\n",
    "\n",
    "path_data_source=dir_path+\"/data/data_source/\"\n",
    "\n",
    "#sub_path_data_source=\"demo_onefile/\"\n",
    "\n",
    "sub_path_data_source=\"demo/\"\n",
    "\n",
    "file_path=path_data_source+sub_path_data_source\n",
    "\n",
    "file_names = [os.path.join(file_path, f) \n",
    "                      for f in os.listdir(file_path) \n",
    "                      if f.endswith(\".txt\")]\n",
    "\n",
    "#sub_path_data_source=\"demo/\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "path_database=dir_path+\"/data/database/csv/\"\n",
    "path_sub_tfidf=\"sub_tfidf/\"\n",
    "path_full_tfidf=\"full_tfidf/\"\n",
    "path_tf=\"sub_tf/\"\n",
    "#path_database='/home/fsg/output_csv/database/'\n",
    "#path_database='/media/fsg/74C86089C8604C04/PHD/Softwares/ph/database/' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XML DataBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#path_XML_database='/media/fsg/74C86089C8604C04/PHD/Softwares/ph/database/XML/' \n",
    "path_XML_database=dir_path+\"/data/database/XML/\"\n",
    "path_freq_word_files=path_XML_database+\"freq/word/\"\n",
    "path_freq_lesk_files=path_XML_database+\"freq/lesk/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_source = 'cs.txt'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Table Data Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TF_File=\"TF_File\"\n",
    "idf=\"idf.csv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XML Data Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''config = tf.ConfigProto(device_count={\"CPU\": 7},\n",
    "                        allow_soft_placement=True,\n",
    "                        inter_op_parallelism_threads=1,\n",
    "                        intra_op_parallelism_threads=1,\n",
    "                        use_per_session_threads=True)'''\n",
    "\n",
    "\n",
    "config = tf.ConfigProto(device_count={\"GPU\": 2,\"CPU\":32},\n",
    "                        allow_soft_placement=True,\n",
    "                        inter_op_parallelism_threads=1,\n",
    "                        intra_op_parallelism_threads=1,\n",
    "                        use_per_session_threads=True,\n",
    "                        log_device_placement=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generic function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write data to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Write Excell sheet\n",
    "'''\n",
    "def save_file_to_database(data_rows,path_database,file_databbase,header_list):\n",
    "    outfile = open(path_database+file_databbase,'w')\n",
    "    writer=csv.writer(outfile)\n",
    "    #header_list=['uuid','paragraph','doc_id']\n",
    "    i=0\n",
    "    for line in data_rows:\n",
    "        row=[i,line,'paragraph no.'+str(i)]\n",
    "        if i==0:\n",
    "            \n",
    "            writer.writerow(header_list)\n",
    "            writer.writerow(row)\n",
    "        else:\n",
    "            #print('ff')\n",
    "            writer.writerow(row)\n",
    "        i+= 1\n",
    "        #outfile.close()\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Read Excell sheet\n",
    "'''\n",
    "def read_text_from_database(path_database,file_databbase):\n",
    "    queue_paragraph=[]\n",
    "    #f = open(sys.argv[1], 'rt')\n",
    "    outfile = open(path_database+file_databbase,'rt')\n",
    "    try:\n",
    "                \n",
    "        reader=csv.reader(outfile)\n",
    "        for row in reader:\n",
    "            queue_paragraph.append(row)\n",
    "            #print (row)\n",
    "    finally:\n",
    "        print (\"row\")\n",
    "        outfile.close()\n",
    "        \n",
    "    return queue_paragraph\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_cvs_by_pands(path_database,file_databbase,index_col, header):\n",
    "    return pd.read_csv(path_database+file_databbase,index_col=index_col,header=header)\n",
    "\n",
    "#read_cvs_by_pands(path_database,paragraph_table,index_col=0,header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def write_cvs_by_pands(path_database,file_databbase,header,data_rows):\n",
    "    csv_df=pd.DataFrame(data_rows,columns=header ) \n",
    "    csv_df.to_csv(path_database+file_databbase)\n",
    "\n",
    "    \n",
    "\n",
    "#write_cvs_by_pands(path_database,sentences_paragraph_table,sentences_paragraph_list,sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save pragraphs to files\n",
    "def write_file(pragraph,num_pragraph,path):\n",
    "    file = open(path+str(num_pragraph)+\".txt\",\"w\") \n",
    " \n",
    "    file.write(pragraph) \n",
    "    \n",
    "    file.close() \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create sub dataset\n",
    "def sub_dataset(path_data_source,data_source):\n",
    "    pragraphs=txt_pragraphs(read_file(path_data_source+data_source))\n",
    "    counter=0\n",
    "    for pragraph in pragraphs:\n",
    "        print('pragraph no ',counter)\n",
    "        write_file(pragraph,counter,sub_path_data_source)\n",
    "        counter +=1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre processing pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Huge File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_file(str):\n",
    "    file = open(str,'r')\n",
    "    txt=file.read()\n",
    "    #print(txt)\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_bz():\n",
    "    #https://codeyarns.com/2013/06/03/how-to-read-a-text-file-compressed-using-bzip2-in-python/\n",
    "    import bz2\n",
    "    bz_file = bz2.BZ2File(\"/media/fsg/74C86089C8604C04/PHD/Dataset/wiki.bz2\")\n",
    "    line_list = bz_file.readlines()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Document to pragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def txt_pragraphs(str):\n",
    "    pragraphs = str.split(\"\\n\\n\")\n",
    "    return pragraphs\n",
    "#pragraphs=txt_pragraphs(txt)\n",
    "#type(pragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save paragraphs in excel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#save_file_to_database(txt_pragraphs(read_file(path_data_source+data_source)),path_database,paragraph_table,paragraph_header_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Split Paragraph to Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pragraph_to_setnences(str):\n",
    "    return sent_tokenize(str)\n",
    "#setnences=pragraph_to_setnences(pragraphs[n_pragraph])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Process For Sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing English stopwords and Punct per Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_stop_words = ['the', 'that', 'to', 'as', 'there', 'has', 'and', 'or', 'is', 'not', 'a', 'of', 'but', 'in', 'by', 'on', 'are', 'it', 'if','what','where','how','when']\n",
    "new_stop_words2=['--','i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now','even','until','then','must']\n",
    "numbers=[1,2,3,4,5,6,7,8,9]\n",
    "#stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "def remove_stopword_sentences(str):\n",
    "    tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
    "    \n",
    "    words=tokenizer.tokenize(str)\n",
    "    \n",
    "    english_stops = set(stopwords.words('english'))\n",
    "    stems=[]\n",
    "    list_word=[word for word in words if word.lower() not in english_stops and word.lower() not in new_stop_words and word.lower() not in new_stop_words2 and  not word.lower().isdigit() and word.lower() not in digits and word.lower() not in  numbers]\n",
    "    \n",
    "    for word in list_word:\n",
    "        #stems.append(stem(word))\n",
    "        #stems.append(PorterStemmer().stem(word))\n",
    "        #stems.append(stemmer.stem(word))\n",
    "        #stems.append(stemmer.stem(\"computer\"))\n",
    "        stems.append(word)\n",
    "    \n",
    "    return stems#(stem(setem_word for setem_word in  ([word for word in words if word not in english_stops and word not in new_stop_words])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#paragraph_list=read_text_from_database(path_database,paragraph_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_list_sentece(pragraph):\n",
    "    words_list=[]\n",
    "    setnences=pragraph_to_setnences(pragraph)\n",
    "    for indexs in range(len(setnences)):    \n",
    "        #print(\"Sentence No. \",indexs,\": \",setnences[indexs],\"\\n\")\n",
    "        words=remove_stopword_sentences(setnences[indexs])\n",
    "        wordsent=''\n",
    "        for index in range(len(words)):\n",
    "            wordsent+=' '+words[index]\n",
    "            #print(\"wordsent:\",wordsent)\n",
    "            \n",
    "        words_list.append(wordsent)\n",
    "        #count = Counter(words)\n",
    "        #print(\"wordsent:\",wordsent)\n",
    "        #print(\" word:\",words)\n",
    "    print(words_list)\n",
    "    return words_list\n",
    "\n",
    "#corpus=word_list_sentece(pragraphs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Sense Disambiguation (WSD): LESK per Sentence\n",
    "\n",
    "Given an ambiguous word and the context in which the word occurs, Lesk returns a Synset with the highest number of overlapping words between the context sentence and different definitions from each Synset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "this function for compute lesk for each word(list of word) in sentence\n",
    "'''\n",
    "def lesk_words_sentence(words,sentence):\n",
    "    lesks= []\n",
    "    for word in words:\n",
    "        if lesk(sentence,word, 'n') is not None:\n",
    "            lesks.append(lesk(sentence,word, 'n'))\n",
    "            #print(\"Word is: \",word,\"\\n LESK: \",lesk(sentence,word, 'n'),\"\\n Sentence: \",sentence )\n",
    "        \n",
    "    return lesks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "this function for compute lesk of word in sentence\n",
    "'''\n",
    "\n",
    "def lesk_word_sentence(sentence,word):\n",
    "    from nltk.wsd import lesk\n",
    "    lesk_synset=''\n",
    "    #lesks= []\n",
    "    #for word in words:\n",
    "    #disambiguated=lesk(context_sentence=sentence, ambiguous_word=word)\n",
    "    disambiguated=lesk(sentence,word, 'n')\n",
    "    #print(disambiguated)\n",
    "    #if disambiguated is not None:\n",
    "    lesk_synset=disambiguated\n",
    "    #else:\n",
    "    #lesk_synset=0\n",
    "    #print(\"Word is: \",word,\"\\n LESK: \",lesk(sentence,word, 'n'),\"\\n Sentence: \",sentence )\n",
    "        \n",
    "    return lesk_synset\n",
    "\n",
    "#lesk(\"Computer science is a discipline that spans theory and practice\",\"science\")\n",
    "\n",
    "#sent = 'people should be able to marry a person of their choice'.split()\n",
    "#lesk(sent, 'able')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating WordNet Synset similarity using information content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import wordnet_ic as wnic\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def similarity_by_infocontent(sense1, sense2, option):\n",
    "    #sense1=\"Synset('\"+sense1+\"')\"\n",
    "    #sense2=\"Synset('\"+sense2+\"')\"\n",
    "    sense1 = wn.synset(sense1)\n",
    "    sense2 = wn.synset(sense2)\n",
    "    print(sense1,sense2)\n",
    "    \"\"\" Returns similarity scores by information content. \"\"\"\n",
    "    #if sense1.pos != sense2.pos: # infocontent sim can't do diff POS.\n",
    "        #return 0\n",
    "\n",
    "    info_contents = ['ic-bnc-add1.dat', 'ic-bnc-resnik-add1.dat', \n",
    "                     'ic-bnc-resnik.dat', 'ic-bnc.dat', \n",
    "\n",
    "                     'ic-brown-add1.dat', 'ic-brown-resnik-add1.dat', \n",
    "                     'ic-brown-resnik.dat', 'ic-brown.dat', \n",
    "\n",
    "                     'ic-semcor-add1.dat', 'ic-semcor.dat',\n",
    "\n",
    "                     'ic-semcorraw-add1.dat', 'ic-semcorraw-resnik-add1.dat', \n",
    "                     'ic-semcorraw-resnik.dat', 'ic-semcorraw.dat', \n",
    "\n",
    "                     'ic-shaks-add1.dat', 'ic-shaks-resnik.dat', \n",
    "                     'ic-shaks-resnink-add1.dat', 'ic-shaks.dat', \n",
    "\n",
    "                     'ic-treebank-add1.dat', 'ic-treebank-resnik-add1.dat', \n",
    "                     'ic-treebank-resnik.dat', 'ic-treebank.dat']\n",
    "\n",
    "    if option in ['res', 'resnik']:\n",
    "        #return wn.res_similarity(sense1, sense2, wnic.ic('ic-bnc-resnik-add1.dat'))\n",
    "        #print('simRe snik (c1,c2) = -log p(lso(c1,c2)) = IC(lso(c1,c2)')\n",
    "        return wn.res_similarity(sense1, sense2, wnic.ic('ic-treebank-resnik-add1.dat'))\n",
    "    #return min(wn.res_similarity(sense1, sense2, wnic.ic(ic)) \\\n",
    "    #             for ic in info_contents)\n",
    "\n",
    "    elif option in ['jcn', \"jiang-conrath\"]:\n",
    "        #return wn.jcn_similarity(sense1, sense2, wnic.ic('ic-bnc-add1.dat'))\n",
    "        #print('sim(jcn) (c1,c2 )= (IC(c1) + IC(c2 )) - 2IC(lso(c1,c2 ))')\n",
    "        return wn.jcn_similarity(sense1, sense2, wnic.ic('ic-treebank.dat'))\n",
    "\n",
    "    elif option in ['lin']:\n",
    "        #return wn.lin_similarity(sense1, sense2, wnic.ic('ic-bnc-add1.dat'))\n",
    "        #print('sim(lin) (c1,c2)=(2IC(lso(c1,c2 )))/(IC(c1)+IC(c2))')\n",
    "        return wn.lin_similarity(sense1, sense2, wnic.ic('ic-treebank.dat'))\n",
    "\n",
    "def sim(sense1, sense2, option=\"path\"):\n",
    "    \"\"\" Calculates similarity based on user's choice. \"\"\"\n",
    "    option = option.lower()\n",
    "    if option.lower() in [\"path\", \"path_similarity\", \n",
    "                        \"wup\", \"wupa\", \"wu-palmer\", \"wu-palmer\",\n",
    "                        'lch', \"leacock-chordorow\"]:\n",
    "        return similarity_by_path(sense1, sense2, option) \n",
    "    elif option.lower() in [\"res\", \"resnik\",\n",
    "                          \"jcn\",\"jiang-conrath\",\n",
    "                          \"lin\"]:\n",
    "        return similarity_by_infocontent(sense1, sense2, option)\n",
    "\n",
    "def max_similarity(context_sentence, ambiguous_word, option=\"path\", \n",
    "                   pos=None, best=True):\n",
    "    \"\"\"\n",
    "    Perform WSD by maximizing the sum of maximum similarity between possible \n",
    "    synsets of all words in the context sentence and the possible synsets of the \n",
    "    ambiguous words (see http://goo.gl/XMq2BI):\n",
    "    {argmax}_{synset(a)}(\\sum_{i}^{n}{{max}_{synset(i)}(sim(i,a))}\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "    for i in wn.synsets(ambiguous_word):\n",
    "        try:\n",
    "            if pos and pos != str(i.pos()):\n",
    "                continue\n",
    "        except:\n",
    "            if pos and pos != str(i.pos):\n",
    "                continue\n",
    "        result[i] = sum(max([sim(i,k,option) for k in wn.synsets(j)]+[0]) \\\n",
    "                        for j in word_tokenize(context_sentence))\n",
    "\n",
    "    if option in [\"res\",\"resnik\"]: # lower score = more similar\n",
    "        result = sorted([(v,k) for k,v in result.items()])\n",
    "    else: # higher score = more similar\n",
    "        result = sorted([(v,k) for k,v in result.items()],reverse=True)\n",
    "    #print (result)\n",
    "    if best: return result[0][1];\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#similarity_by_infocontent('able.s.04','able.s.04','res')\n",
    "\n",
    "#sim2=wn.jcn_similarity(wn.synset('able.s.04'),wn.synset('able.s.04'),'ic-treebank-resnik-add1.data')\n",
    "#sim2\n",
    "#wn.synset('able.s.04').res_similarity(wn.synset('able.s.04'), 'ic-treebank-resnik-add1.data')\n",
    "\n",
    "#wn.res_similarity(wn.synset('able.n.04'), wn.synset('adept.n.01'), wnic.ic('ic-treebank-resnik-add1.dat'))\n",
    "\n",
    "\n",
    "sense1 = wn.synset('ace.n.03')\n",
    "sense2 = wn.synset('acquiring.n.01')\n",
    "print(sense1,sense2)\n",
    "wn.res_similarity(sense1,sense2,wnic.ic('ic-treebank-resnik-add1.dat'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simlarty per Words for Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "calculate simantic simelart for Dimensionality reduction vector\n",
    "say vector is n element [n1,n2,n3,.....nm], data frame row=n,col=n\n",
    "sim(n[row],n[col])if if row != col:\n",
    "option is sim method like res,lin,jcn ...... for IC\n",
    "\n",
    "'''\n",
    "#print(type(tfidf_feature_names))\n",
    "#tfidf_feature_names\n",
    "\n",
    "def sim_docs_lesk(df_freq,option):\n",
    "    \n",
    "    series=list(df_freq)#pd.Series(data=lesk_vec)\n",
    "    #series.drop_duplicates()\n",
    "    synset_lesk_noDuplicates= series#.tolist()    \n",
    "    #df_all_synset_lesk = pd.DataFrame(index=series, columns=series )\n",
    "    df_all_synset_lesk = pd.DataFrame(columns=series )\n",
    "    #print (\"synset_lesk_noDuplicates\",len(synset_lesk_noDuplicates))\n",
    "    \n",
    "    for row in range(len(synset_lesk_noDuplicates)):\n",
    "        try:\n",
    "            data_row=[]\n",
    "            for col in range(len(synset_lesk_noDuplicates)):\n",
    "\n",
    "                    #if row < col:\n",
    "\n",
    "                    sim=similarity_by_infocontent(series[row],series[col],option)\n",
    "                    if sim is not None:\n",
    "                        data_row.append(sim)\n",
    "                    else:\n",
    "                        data_row.append(0)\n",
    "                        #print(len(data_row))\n",
    "\n",
    "\n",
    "                    #else:\n",
    "\n",
    "                        #data_row.append(0)\n",
    "                        #print(len(data_row))\n",
    "\n",
    "            #print(series[row])\n",
    "            df_all_synset_lesk.loc[series[row]]=data_row\n",
    "        except  Exception as inst:\n",
    "                data_row.append(0)\n",
    "                print(type(inst))    # the exception instance\n",
    "                print(inst.args)     # arguments stored in .args\n",
    "                print(inst)          # __str__ allows args to be printed directly,\n",
    "                pass\n",
    "                #print(\"Ex\")\n",
    "    return df_all_synset_lesk\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_row_csv(path_database,idf,list_data):\n",
    "    with open(path_database+idf, 'a') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(list_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def name_file(full_name_path):\n",
    "    d=full_name_path.split(\"/\")\n",
    "    print(d)\n",
    "    name=d[len(d)-1].split(\".\")\n",
    "    return name[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'file_names' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-82-6639d6c9ccdf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mindex_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfile_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'file_names' is not defined"
     ]
    }
   ],
   "source": [
    "#with tf.control_dependencies(paragraph_list):\n",
    "                \n",
    "with tf.Session(config=config) as sess:\n",
    "        index_paragraph=0\n",
    "        col=1\n",
    "                \n",
    "        index_file=0\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        #print(file_names)\n",
    "        \n",
    "        for filename in file_names:\n",
    "            #print(\"index_file\",str(index_file))\n",
    "            word_file_fatma=[]\n",
    "            with open(filename) as inf:\n",
    "                #print(\"tpe\",type(inf))\n",
    "                txt=inf.read()\n",
    "                \n",
    "                paragraph_list=txt_pragraphs(txt)   \n",
    "        \n",
    "        \n",
    "                for paragraph in paragraph_list: #get pragraphs(documents) from DB\n",
    "                    #print(\"Pragraph type \",type(paragraph))\n",
    "                   \n",
    "                    \n",
    "                    if index_paragraph ==0:\n",
    "                        index_paragraph += 1\n",
    "                    else:\n",
    "                        \n",
    "                        setnences=pragraph_to_setnences(paragraph)#partitions paragraph to sentence\n",
    "                        \n",
    "                        \n",
    "                        for setnence in setnences:\n",
    "                            #print(\"  \",setnence)                            \n",
    "\n",
    "                            words=remove_stopword_sentences(setnence)#remove stop words and noise\n",
    "\n",
    "                            for word in words:\n",
    "\n",
    "                                \n",
    "                                lesk=lesk_word_sentence(setnence,word)#get LESK of word in sentence\n",
    "\n",
    "                                #paragraph_word.append(word_sentence)\n",
    "\n",
    "                                if lesk is not None:\n",
    "                                    \n",
    "                                   \n",
    "                                    word_file_fatma.append(lesk.name())\n",
    "                                \n",
    "\n",
    "                            \n",
    "                            '''////////////////END Sentence////////////////# '''\n",
    "                            \n",
    "                        \n",
    "                        #write_cvs_by_pands(path_database,word_sentences_table,word_sentences_list,word_sentences_list_data)\n",
    "\n",
    "                       \n",
    "                        '''////////////////END PARAGRAPH////////////////# '''\n",
    "\n",
    "                #write_cvs_by_pands(path_database,sentences_paragraph_table,sentences_paragraph_list,sentences_paragraph_list_data)\n",
    "\n",
    "                #print(word_file_fatma)\n",
    "                \n",
    "                word_file_Freq=Counter(word_file_fatma)\n",
    "                sum_count=sum(word_file_Freq.values())\n",
    "\n",
    "                #print(type(word_file_Freq))\n",
    "                #print(word_file_Freq)\n",
    "                #csv_df=pd.DataFrame([word_file_Freq],columns=word_file_Freq.keys() ) \n",
    "                freq=[]\n",
    "                for i in word_file_Freq.values():\n",
    "                    c=i/sum_count\n",
    "                    freq.append(c)\n",
    "                csv_df=pd.DataFrame([freq],columns=word_file_Freq.keys() ) \n",
    "                \n",
    "                \n",
    "                csv_df.to_csv(path_database+path_tf+TF_File+name_file(filename)+\".csv\")\n",
    "                # add to idf file \n",
    "                \n",
    "                full_list=[]\n",
    "                full_list.insert(0,name_file(filename))\n",
    "                ss=full_list+list(word_file_Freq.keys())\n",
    "                # add to single\n",
    "                \n",
    "                add_row_csv(path_database+path_sub_tfidf,name_file(filename)+\".csv\",ss)\n",
    "                # add to total idf file \n",
    "                add_row_csv(path_database+path_full_tfidf,idf,ss)\n",
    "                index_file +=1\n",
    "                #calculate semantic semlarty by res\n",
    "                #df_sim_sem_lesk=sim_docs_lesk(cv_tfidf_lesk.get_feature_names(),'res')\n",
    "                #df_sim_sem_lesk.to_csv(path_database+res_sem_sim_table)'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
