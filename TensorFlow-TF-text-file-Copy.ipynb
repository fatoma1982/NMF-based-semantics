{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#! /usr/bin/python3.5\n",
    "#https://learningtensorflow.com/ReadingFilesBasic/\n",
    "#https://www.tensorflow.org/tutorials/using_gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# To compute term frequancy from multi txt file and save it in multi csv and one csv by parellel process using tensorflow on GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import csv\n",
    "from tensorflow.python.client import timeline\n",
    "import nltk\n",
    "from nltk.wsd import lesk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.wsd import lesk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import PorterStemmer\n",
    "from nltk.stem.porter import *\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from string import digits\n",
    "#from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pathes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dir_path = os.path.dirname(os.path.realpath(__file__))\n",
    "\n",
    "path_data_source=dir_path+\"/data/data_source/\"\n",
    "\n",
    "#sub_path_data_source=\"demo_onefile/\"\n",
    "\n",
    "sub_path_data_source=\"demo/\"\n",
    "\n",
    "file_path=path_data_source+sub_path_data_source\n",
    "\n",
    "file_names = [os.path.join(file_path, f) \n",
    "                      for f in os.listdir(file_path) \n",
    "                      if f.endswith(\".txt\")]\n",
    "\n",
    "#sub_path_data_source=\"demo/\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "path_database=dir_path+\"/data/database/csv/\"\n",
    "path_sub_tfidf=\"sub_tfidf/\"\n",
    "path_full_tfidf=\"full_tfidf/\"\n",
    "path_tf=\"sub_tf/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_source = 'cs.txt'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Table Data Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TF_File=\"TF-\"\n",
    "TF_Full=\"TF-Full.csv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''config = tf.ConfigProto(device_count={\"CPU\": 7},\n",
    "                        allow_soft_placement=True,\n",
    "                        inter_op_parallelism_threads=1,\n",
    "                        intra_op_parallelism_threads=1,\n",
    "                        use_per_session_threads=True)'''\n",
    "#import pprint\n",
    "\n",
    "\n",
    "\n",
    "config = tf.ConfigProto(device_count={\"GPU\": 2,\"CPU\":32},\n",
    "                        allow_soft_placement=True,\n",
    "                        inter_op_parallelism_threads=32,\n",
    "                        intra_op_parallelism_threads=32,\n",
    "                        use_per_session_threads=True,\n",
    "                        log_device_placement=True)\n",
    "\n",
    "#pprint.pprint({ k:str(getattr(config,k)) for k in sorted(dir(config)) if not k.startswith('_')})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generic function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write data to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Write Excell sheet\n",
    "'''\n",
    "def save_file_to_database(data_rows,path_database,file_databbase,header_list):\n",
    "    outfile = open(path_database+file_databbase,'w')\n",
    "    writer=csv.writer(outfile)\n",
    "    #header_list=['uuid','paragraph','doc_id']\n",
    "    i=0\n",
    "    for line in data_rows:\n",
    "        row=[i,line,'paragraph no.'+str(i)]\n",
    "        if i==0:\n",
    "            \n",
    "            writer.writerow(header_list)\n",
    "            writer.writerow(row)\n",
    "        else:\n",
    "            #print('ff')\n",
    "            writer.writerow(row)\n",
    "        i+= 1\n",
    "        #outfile.close()\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Read Excell sheet\n",
    "'''\n",
    "def read_text_from_database(path_database,file_databbase):\n",
    "    queue_paragraph=[]\n",
    "    #f = open(sys.argv[1], 'rt')\n",
    "    outfile = open(path_database+file_databbase,'rt')\n",
    "    try:\n",
    "                \n",
    "        reader=csv.reader(outfile)\n",
    "        for row in reader:\n",
    "            queue_paragraph.append(row)\n",
    "            #print (row)\n",
    "    finally:\n",
    "        print (\"row\")\n",
    "        outfile.close()\n",
    "        \n",
    "    return queue_paragraph\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_cvs_by_pands(path_database,file_databbase,index_col, header):\n",
    "    return pd.read_csv(path_database+file_databbase,index_col=index_col,header=header)\n",
    "\n",
    "#read_cvs_by_pands(path_database,paragraph_table,index_col=0,header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def write_cvs_by_pands(path_database,file_databbase,header,data_rows):\n",
    "    csv_df=pd.DataFrame(data_rows,columns=header ) \n",
    "    csv_df.to_csv(path_database+file_databbase)\n",
    "\n",
    "    \n",
    "\n",
    "#write_cvs_by_pands(path_database,sentences_paragraph_table,sentences_paragraph_list,sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save pragraphs to files\n",
    "def write_file(pragraph,num_pragraph,path):\n",
    "    file = open(path+str(num_pragraph)+\".txt\",\"w\") \n",
    " \n",
    "    file.write(pragraph) \n",
    "    \n",
    "    file.close() \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create sub dataset\n",
    "def sub_dataset(path_data_source,data_source):\n",
    "    pragraphs=txt_pragraphs(read_file(path_data_source+data_source))\n",
    "    counter=0\n",
    "    for pragraph in pragraphs:\n",
    "        print('pragraph no ',counter)\n",
    "        write_file(pragraph,counter,sub_path_data_source)\n",
    "        counter +=1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre processing pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Huge File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_file(str):\n",
    "    file = open(str,'r')\n",
    "    txt=file.read()\n",
    "    #print(txt)\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Document to pragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def txt_pragraphs(str):\n",
    "    pragraphs = str.split(\"\\n\\n\")\n",
    "    return pragraphs\n",
    "#pragraphs=txt_pragraphs(txt)\n",
    "#type(pragraphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save paragraphs in excel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#save_file_to_database(txt_pragraphs(read_file(path_data_source+data_source)),path_database,paragraph_table,paragraph_header_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Split Paragraph to Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pragraph_to_setnences(str):\n",
    "    return sent_tokenize(str)\n",
    "#setnences=pragraph_to_setnences(pragraphs[n_pragraph])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Process For Sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing English stopwords and Punct per Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_stop_words = ['the', 'that', 'to', 'as', 'there', 'has', 'and', 'or', 'is', 'not', 'a', 'of', 'but', 'in', 'by', 'on', 'are', 'it', 'if','what','where','how','when']\n",
    "new_stop_words2=['--','i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now','even','until','then','must']\n",
    "numbers=[1,2,3,4,5,6,7,8,9]\n",
    "#stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "def remove_stopword_sentences(str):\n",
    "    tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
    "    \n",
    "    words=tokenizer.tokenize(str)\n",
    "    \n",
    "    english_stops = set(stopwords.words('english'))\n",
    "    stems=[]\n",
    "    list_word=[word for word in words if word.lower() not in english_stops and word.lower() not in new_stop_words and word.lower() not in new_stop_words2 and  not word.lower().isdigit() and word.lower() not in digits and word.lower() not in  numbers]\n",
    "    \n",
    "    for word in list_word:\n",
    "        #stems.append(stem(word))\n",
    "        #stems.append(PorterStemmer().stem(word))\n",
    "        #stems.append(stemmer.stem(word))\n",
    "        #stems.append(stemmer.stem(\"computer\"))\n",
    "        stems.append(word)\n",
    "    \n",
    "    return stems#(stem(setem_word for setem_word in  ([word for word in words if word not in english_stops and word not in new_stop_words])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_list_sentece(pragraph):\n",
    "    words_list=[]\n",
    "    setnences=pragraph_to_setnences(pragraph)\n",
    "    for indexs in range(len(setnences)):    \n",
    "        #print(\"Sentence No. \",indexs,\": \",setnences[indexs],\"\\n\")\n",
    "        words=remove_stopword_sentences(setnences[indexs])\n",
    "        wordsent=''\n",
    "        for index in range(len(words)):\n",
    "            wordsent+=' '+words[index]\n",
    "            #print(\"wordsent:\",wordsent)\n",
    "            \n",
    "        words_list.append(wordsent)\n",
    "        #count = Counter(words)\n",
    "        #print(\"wordsent:\",wordsent)\n",
    "        #print(\" word:\",words)\n",
    "    print(words_list)\n",
    "    return words_list\n",
    "\n",
    "#corpus=word_list_sentece(pragraphs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Sense Disambiguation (WSD): LESK per Sentence\n",
    "\n",
    "Given an ambiguous word and the context in which the word occurs, Lesk returns a Synset with the highest number of overlapping words between the context sentence and different definitions from each Synset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "this function for compute lesk for each word(list of word) in sentence\n",
    "'''\n",
    "def lesk_words_sentence(words,sentence):\n",
    "    lesks= []\n",
    "    for word in words:\n",
    "        if lesk(sentence,word, 'n') is not None:\n",
    "            lesks.append(lesk(sentence,word, 'n'))\n",
    "            #print(\"Word is: \",word,\"\\n LESK: \",lesk(sentence,word, 'n'),\"\\n Sentence: \",sentence )\n",
    "        \n",
    "    return lesks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "this function for compute lesk of word in sentence\n",
    "'''\n",
    "\n",
    "def lesk_word_sentence(sentence,word):\n",
    "    from nltk.wsd import lesk\n",
    "    lesk_synset=''\n",
    "    #lesks= []\n",
    "    #for word in words:\n",
    "    #disambiguated=lesk(context_sentence=sentence, ambiguous_word=word)\n",
    "    disambiguated=lesk(sentence,word, 'n')\n",
    "    #print(disambiguated)\n",
    "    #if disambiguated is not None:\n",
    "    lesk_synset=disambiguated\n",
    "    #else:\n",
    "    #lesk_synset=0\n",
    "    #print(\"Word is: \",word,\"\\n LESK: \",lesk(sentence,word, 'n'),\"\\n Sentence: \",sentence )\n",
    "        \n",
    "    return lesk_synset\n",
    "\n",
    "#lesk(\"Computer science is a discipline that spans theory and practice\",\"science\")\n",
    "\n",
    "#sent = 'people should be able to marry a person of their choice'.split()\n",
    "#lesk(sent, 'able')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_row_csv(path_database,idf,list_data):\n",
    "    with open(path_database+idf, 'a') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(list_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def name_file(full_name_path):\n",
    "    d=full_name_path.split(\"/\")\n",
    "    print(d)\n",
    "    name=d[len(d)-1].split(\".\")\n",
    "    return name[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "                \n",
    "with tf.Session(config=config) as sess:\n",
    "        index_paragraph=0\n",
    "        col=1\n",
    "                \n",
    "        index_file=0\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        #print(file_names)\n",
    "        \n",
    "        for filename in file_names:\n",
    "            #print(\"index_file\",str(index_file))\n",
    "            word_file_fatma=[]\n",
    "            with open(filename) as inf:\n",
    "                #print(\"tpe\",type(inf))\n",
    "                txt=inf.read()\n",
    "                \n",
    "                paragraph_list=txt_pragraphs(txt)   \n",
    "        \n",
    "        \n",
    "                for paragraph in paragraph_list: #get pragraphs(documents) from DB\n",
    "                    #print(\"Pragraph type \",type(paragraph))\n",
    "                   \n",
    "                    \n",
    "                    if index_paragraph ==0:\n",
    "                        index_paragraph += 1\n",
    "                    else:\n",
    "                        \n",
    "                        setnences=pragraph_to_setnences(paragraph)#partitions paragraph to sentence\n",
    "                        \n",
    "                        \n",
    "                        for setnence in setnences:\n",
    "                            #print(\"  \",setnence)                            \n",
    "\n",
    "                            words=remove_stopword_sentences(setnence)#remove stop words and noise\n",
    "\n",
    "                            for word in words:\n",
    "\n",
    "                                \n",
    "                                lesk=lesk_word_sentence(setnence,word)#get LESK of word in sentence\n",
    "\n",
    "                                #paragraph_word.append(word_sentence)\n",
    "\n",
    "                                if lesk is not None:\n",
    "                                    \n",
    "                                   \n",
    "                                    word_file_fatma.append(lesk.name())\n",
    "                                \n",
    "\n",
    "                            \n",
    "                            '''////////////////END Sentence////////////////# '''\n",
    "                            \n",
    "                        \n",
    "                       \n",
    "\n",
    "                       \n",
    "                        '''////////////////END PARAGRAPH////////////////# '''\n",
    "\n",
    "                \n",
    "                \n",
    "                word_file_Freq=Counter(word_file_fatma)\n",
    "                sum_count=sum(word_file_Freq.values())\n",
    "\n",
    "                \n",
    "                freq=[]\n",
    "                for i in word_file_Freq.values():\n",
    "                    c=i/sum_count\n",
    "                    freq.append(c)\n",
    "                csv_df=pd.DataFrame([freq],columns=word_file_Freq.keys() ) \n",
    "                \n",
    "                \n",
    "                csv_df.to_csv(path_database+path_tf+TF_File+name_file(filename)+\".csv\")\n",
    "                # add to idf file \n",
    "                \n",
    "                full_list=[]\n",
    "                full_list.insert(0,name_file(filename))\n",
    "                full_list=full_list+list(word_file_Freq.keys())\n",
    "                # add to single\n",
    "                \n",
    "                add_row_csv(path_database+path_sub_tfidf,name_file(filename)+\".csv\",full_list)\n",
    "                # add to total idf file \n",
    "                add_row_csv(path_database+path_full_tfidf,TF_Full,full_list)\n",
    "                index_file +=1\n",
    "                "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
